## Calculating limits
limit_inf <- num
limit_upp <- qf
## Calculating w value (Difference between upper limit and lower limit)
w <- limit_upp - limit_inf
## Storing w data
l <- c(l, w)
## Storing auxiliar data
qiaux <- c(qiaux, num)
qsaux <- c(qsaux, qf)
}
## Getting the min amplitude interval
min_amplitude <- min(l)
## Getting index of the min amplitude interval
min_index <- which.min(l)
## Getting the limits of the interval
limit_inf_f <- qiaux[min_index]
limit_upp_f <- qsaux[min_index]
## Storing on the table
chi_table[[length(chi_table) +1]] <- list(Degrees_of_freedom = n, CI = ci, qi = limit_inf_f, qs = limit_upp_f, w = min_amplitude)
}
}
return(chi_table)
}
# Using our function
chi_ta2 <- chi_table_function_MINw(degreesF_vector, confidence_intervals)
# Visualizing results
print(chi_ta2)
View(chi_ta2)
## Modern Multivariate Statistical Techniques (Izenman) Ex. 8.2
library(MASS)
library(ggplot2)
library(GGally)
install.packages("GGally")
## Modern Multivariate Statistical Techniques (Izenman) Ex. 8.5
# Loading dataset
load("C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_01/diabetes.rda")
# Parsing to dataframe
diabetes_df <- as.data.frame(diabetes)
# Checking head dataset
head(diabetes_df)
# Scatterplot matrix of all five variables, colored by class
ggpairs(diabetes_df, columns = 1:5, aes(color = class)) +
labs(title = "Scatterplot Matrix of Diabetes Data")
data(eurodist)
eurodist
install.packages("knitr")
install.packages("kableExtra")
install.packages('tinytex')
tinytex::install_tinytex()
tinytex::pdflatex('Assignment01.tex')
library(tidyverse)
library(palmerpenguins)
set.seed(9450)
penguin_df<-
palmerpenguins::penguins %>%
na.omit()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
lin_reg <- lm(bill_depth_mm ~ bill_length_mm, data=penguin_df)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
summary(lin_reg)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm,
color=species)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
chin<-
penguin_df %>%
filter(species == "Chinstrap")
adelie<-
penguin_df %>%
filter(species == "Adelie")
gentoo<-
penguin_df %>%
filter(species == "Gentoo")
lm_chin<- lm(data=chin, bill_depth_mm ~ bill_length_mm)
lm_adelie<- lm(data=adelie, bill_depth_mm ~ bill_length_mm)
lm_gentoo<- lm(data=gentoo, bill_depth_mm ~ bill_length_mm)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm,
color=species)) +
geom_point() +
geom_abline(slope = lm_chin$coefficients[[2]],
intercept = lm_chin$coefficients[[1]],
color="black") +
geom_abline(slope = lm_adelie$coefficients[[2]],
intercept = lm_adelie$coefficients[[1]],
color="black") +
geom_abline(slope = lm_gentoo$coefficients[[2]],
intercept = lm_gentoo$coefficients[[1]],
color="black") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
library(tidyverse)
library(palmerpenguins)
set.seed(9450)
penguin_df<-
palmerpenguins::penguins %>%
na.omit()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
lin_reg <- lm(bill_depth_mm ~ bill_length_mm, data=penguin_df)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
# Crear el histograma
histograma <- ggplot(data, aes(x = performance_idx)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Histograma de performance index", x = "performance index", y = "Frecuencia") +
theme_minimal()
# Leer el archivo
data <- read.csv("C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS LINEALES/REPOSITORIO/LINEAR_METHODS_ASSIGNMENTS_FALL2024/PROJECT/Student_Performance.csv")
# Ver las primeras filas del DataFrame
head(data)
# Nombres originales de las columnas
colnames(data) <- c("Hours.Studied", "Previous.Scores", "Extracurricular.Activities",
"Sleep.Hours", "Sample.Question.Papers.Practiced", "Performance.Index")
# Renombrar las columnas (estandarizando los nombres de las columnas)
colnames(data) <- tolower(colnames(data))
colnames(data) <- gsub("\\.", "_", colnames(data))
# Acortar los nombres de las columnas
colnames(data) <- c("hrs_studied", "prev_scores", "xtr_activities",
"sleep_hrs", "sample_questions", "performance_idx")
# Verificar los nuevos nombres
print(colnames(data))
# Funcion de analisis univariado
univar_analisis <- function(data) {
results <- list()
for (feature in colnames(data)) {
data_type <- class(data[[feature]])[1]
total <- nrow(data)
nan_count <- sum(is.na(data[[feature]]))
no_missings <- total - nan_count
pct_missings <- nan_count / total
if (is.numeric(data[[feature]])) {
promedio <- round(mean(data[[feature]], na.rm = TRUE),2)
desv_estandar <- round(sd(data[[feature]], na.rm = TRUE),2)
varianza <- round(var(data[[feature]], na.rm = TRUE),2)
minimo <- min(data[[feature]], na.rm = TRUE)
p10 <- quantile(data[[feature]], 0.10, na.rm = TRUE)
q1 <- quantile(data[[feature]], 0.25, na.rm = TRUE)
mediana <- quantile(data[[feature]], 0.50, na.rm = TRUE)
q3 <- quantile(data[[feature]], 0.75, na.rm = TRUE)
p90 <- quantile(data[[feature]], 0.90, na.rm = TRUE)
p95 <- quantile(data[[feature]], 0.95, na.rm = TRUE)
p99 <- quantile(data[[feature]], 0.99, na.rm = TRUE)
maximo <- max(data[[feature]], na.rm = TRUE)
inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] > 0)
neg_inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] < 0)
} else {
promedio <- NA
desv_estandar <- NA
varianza <- NA
minimo <- NA
p1 <- NA
p5 <- NA
p10 <- NA
q1 <- NA
mediana <- NA
q3 <- NA
p90 <- NA
p95 <- NA
p99 <- NA
maximo <- NA
inf_count <- 0
neg_inf_count <- 0
}
results[[length(results) + 1]] <- list(
Variable = feature,
Total = total,
No_Missings = no_missings,
Missings = nan_count,
Pct_Missings = pct_missings,
Promedio = promedio,
Desv_std = desv_estandar,
Varianza = varianza,
Minimo = minimo,
p10 = p10,
q1 = q1,
Mediana = mediana,
q3 = q3,
p90 = p90,
p95 = p95,
p99 = p99,
Maximo = maximo
)
}
result_df <- do.call(rbind, lapply(results, as.data.frame))
rownames(result_df) <- NULL
return(result_df)
}
# Ejecutar la función de análisis univariante
resultados <- univar_analisis(data)
# Crear el histograma
histograma <- ggplot(data, aes(x = performance_idx)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Histograma de performance index", x = "performance index", y = "Frecuencia") +
theme_minimal()
head(data)
hea(data)
head(data)
View(data)
# Analisis de Linealidad y Homocedasticidad
modelo <- lm(data = data, formula = performance_idx ~ prev_score)
# Analisis de Linealidad y Homocedasticidad
modelo <- lm(data = data, formula = performance_idx ~ prev_scores)
ggplot(data = data) +
aes(x = modelo$fitted.values, y = modelo$residuals) +
geom_vline(xintercept = median(modelo$fitted.values)) +
geom_point() +
theme_classic()
# Leyendo archivo
file_path <- "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS LINEALES/REPOSITORIO/LINEAR_METHODS_ASSIGNMENTS_FALL2024/ASSIGNMENT 2/datos_ejercicio_2_tarea_3.txt"
# Leyendo el archivo con el delimitador '|'
datos <- read.delim(file_path, sep = "|")
head(datos)
# Ajustar el modelo de regresión lineal
modelo <- lm(BMI18 ~ Sexo + WT2 + HT2 + WT9 + HT9 + LG9 + ST9 + WT18 + HT18 + LG18 + ST18 + Soma, data = datos)
# Ajustar el modelo de regresión lineal
modelo <- lm(BMI18 ~ Sex + WT2 + HT2 + WT9 + HT9 + LG9 + ST9 + WT18 + HT18 + LG18 + ST18 + Soma, data = datos)
# Mostrar los coeficientes estimados
summary(modelo)$coefficients
# Obtener el resumen del modelo
summary_model <- summary(modelo)
# Extraer R2 y R2 ajustado
r_squared <- summary_model$r.squared
adj_r_squared <- summary_model$adj.r.squared
# Ver la significancia de los coeficientes
coef_significance <- summary_model$coefficients
# Valores para la prediccion
nuevo_sujeto <- data.frame(
Sex = NA,
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71,
WT18 = NA,
HT18 = NA,
LG18 = NA,
ST18 = NA,
Soma = NA
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Valores para la prediccion
nuevo_sujeto <- data.frame(
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Valores para la prediccion
nuevo_sujeto <- data.frame(
Sex = 0,
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71,
WT18 = 0,
HT18 = 0,
LG18 = 0,
ST18 = 0,
Soma = 0
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Mostrar la predicción y el intervalo de confianza
prediccion
setwd("~/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/EXAM 02")
# Ruta al archivo
file_path <- "C:\\Users\\mauva\\OneDrive\\Documents\\ITAM\\9no Semestre\\METODOS MULTIVARIADOS\\REPOSITORIO\\Multivariate_Statistical_Course_Assignments_Fall2024\\EXAM 02\\turtles.rda"
# Carga de los datos
load(file_path)
View(turtles)
# Revision de los datos
head(turtles)
# Analisis univariado
univar_analisis <- function(data) {
results <- list()
for (feature in colnames(data)) {
data_type <- class(data[[feature]])[1]
total <- nrow(data)
nan_count <- sum(is.na(data[[feature]]))
no_missings <- total - nan_count
pct_missings <- nan_count / total
if (is.numeric(data[[feature]])) {
promedio <- round(mean(data[[feature]], na.rm = TRUE),2)
desv_estandar <- round(sd(data[[feature]], na.rm = TRUE),2)
varianza <- round(var(data[[feature]], na.rm = TRUE),2)
minimo <- min(data[[feature]], na.rm = TRUE)
p10 <- quantile(data[[feature]], 0.10, na.rm = TRUE)
q1 <- quantile(data[[feature]], 0.25, na.rm = TRUE)
mediana <- quantile(data[[feature]], 0.50, na.rm = TRUE)
q3 <- quantile(data[[feature]], 0.75, na.rm = TRUE)
p90 <- quantile(data[[feature]], 0.90, na.rm = TRUE)
p95 <- quantile(data[[feature]], 0.95, na.rm = TRUE)
p99 <- quantile(data[[feature]], 0.99, na.rm = TRUE)
maximo <- max(data[[feature]], na.rm = TRUE)
inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] > 0)
neg_inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] < 0)
} else {
promedio <- NA
desv_estandar <- NA
varianza <- NA
minimo <- NA
p1 <- NA
p5 <- NA
p10 <- NA
q1 <- NA
mediana <- NA
q3 <- NA
p90 <- NA
p95 <- NA
p99 <- NA
maximo <- NA
inf_count <- 0
neg_inf_count <- 0
}
results[[length(results) + 1]] <- list(
Variable = feature,
Total = total,
No_Missings = no_missings,
Missings = nan_count,
Pct_Missings = pct_missings,
Promedio = promedio,
Desv_Std = desv_estandar,
Varianza = varianza,
Minimo = minimo,
p10 = p10,
q1 = q1,
Mediana = mediana,
q3 = q3,
p90 = p90,
p95 = p95,
p99 = p99,
Maximo = maximo
)
}
result_df <- do.call(rbind, lapply(results, as.data.frame))
rownames(result_df) <- NULL
return(result_df)
}
# Ejecucion de lafuncion
resultados <- univar_analisis(turtles)
resultados
# Crear el histograma
histograma <- ggplot(turtles, aes(x = length)) +
geom_histogram(binwidth = 10, fill = "green", color = "black", alpha = 0.7) +
labs(title = "Histograma de las longitudes de las tortugas", x = "length", y = "Frecuencia") +
theme_minimal()+
theme(
plot.title = element_text(size = 9),
axis.title = element_text(size = 8),
axis.text = element_text(size = 7)
)
sex_counts <- table(turtles$sex)
sex_counts
install.packages('reshape2')
install.packages("corrplot")
install.packages("ggcorrplot")
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, fig.width=10)
# Libraries
library(MASS)
library(ggplot2)
library(GGally)
library(mclust)
library(kableExtra)
library(FactoMineR)
library(knitr)
library(caret)
library(dplyr)
library(gridExtra)
library(reshape2)
library(corrplot)
library(ggcorrplot)
# Dataset reducido
numerical_vars <- turtles[, c("length", "width", "height")]
# Creando el pairs plot
pairs_plot <- ggpairs(numerical_vars) +
theme_minimal()
# Calcular la matriz de correlación
cor_matrix <- round(cor(numerical_vars), 2)
cor_melted <- melt(cor_matrix)
# Gráfico de la matriz de correlación
cor_plot <- ggcorrplot(cor_matrix,
hc.order = TRUE,
type = "lower",
lab = TRUE)
# Grid
grid.arrange(pairs_plot, cor_plot, ncol = 2)
# Gráfico de la matriz de correlación
cor_plot <- ggcorrplot(cor_matrix,
hc.order = TRUE,
type = "lower",
lab = TRUE)
cor_plot
cor_matrix
# Gráfico de la matriz de correlación
cor_plot <- ggplot(cor_melted, aes(Var1, Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
labs(title = "Matriz de Correlación", x = "", y = "") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
cor_plot
pairs_plot
# Creando el pairs plot
ggpairs(
numerical_vars,
lower = list(continuous = "smooth"),
diag = list(continuous = "barDiag"),
upper = list(continuous = "cor"),
aes(color = length > 120, alpha = 0.7)
) +
theme_minimal() +
theme(
plot.title = element_text(size = 14, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "top"
) +
labs(title = "Analisis de correlación", color = "Longitud > 120")
sex_counts
install.packages('ggfortify')
summary(pca_result)
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, fig.width=10)
# Libraries
library(MASS)
library(ggplot2)
library(GGally)
library(mclust)
library(kableExtra)
library(FactoMineR)
library(knitr)
library(caret)
library(dplyr)
library(gridExtra)
library(ggfortify)
# Especifica la ruta al archivo
file_path <- "C:\\Users\\mauva\\OneDrive\\Documents\\ITAM\\9no Semestre\\METODOS MULTIVARIADOS\\REPOSITORIO\\Multivariate_Statistical_Course_Assignments_Fall2024\\EXAM 02\\turtles.rda"
# Carga el archivo .rda
load(file_path)
# Variables numéricas
numerical_vars <- turtles[, c("length", "width", "height")]
# PCA
pca_result <- prcomp(numerical_vars, scale. = TRUE)
# Resumen del PCA
#summary(pca_result)
# Añadir la variable 'sex' para colorear por sexo
projected_data <- data.frame(pca_result$x, sex = turtles$sex)
# Gráfico
ggplot(projected_data, aes(x = PC1, y = PC2, color = sex)) +
geom_point(size = 3, alpha = 0.7) +
labs(
title = "Primeras 2 Componentes Principales",
x = "Primera Componente Principal (PC1)",
y = "Segunda Componente Principal (PC2)",
color = "Sexo"
) +
theme_minimal() +
theme(
plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "right"
)
summary(pca_result)
