---
title: 'Segundo Examen Parcial: Otoño 2024'
author: "Mauricio Vazquez Moran (000191686)"
date: '2024-11-28'
output:
  pdf_document: default
  html_document: default
---

***Link del repositorio de GitHub: https://github.com/MauricioVazquezM/Multivariate_Statistical_Course_Assignments_Fall2024***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, fig.width=10)

# Libraries
library(MASS)  
library(ggplot2)
library(GGally)
library(mclust)
library(kableExtra)
library(FactoMineR)
library(knitr)
library(caret)
library(dplyr)
library(gridExtra)
library(ggfortify)
library(copula)
```

## Q1: Analisis de componentes principales (PCA)

Como podemos observar en el ***Anexo 2***, nuestro dataset (turtles.rda) no contiene una gran cantidad de variables. Las variables presentes en el conjunto de datos son 'length', 'width', 'height' y 'sex'. Si bien es cierto que un Análisis de Componentes Principales (PCA) se utiliza principalmente como técnica de reducción de dimensionalidad (Izenman 2013), también es cierto que esta herramienta estadística es usada para eliminar la redundancia en los datos, representándolos en términos de nuevas variables (componentes principales) que están descorrelacionadas entre sí. En pocas palabras, el objetivo principal del PCA es resumir un conjunto de variables en un número menor de variables significativas que, colectivamente, expliquen la mayor parte de la variabilidad presente en el conjunto original de datos (James et al., 2017). Bajo nuestra línea de análisis, según el ***Anexo 2***, mediante un Análisis Exploratorio de Datos (EDA), nos damos cuenta de que esta es nuestra situación. Las variables del conjunto de datos mencionado están altamente correlacionadas entre sí. 

Por otro lado, otro objetivo, además de reducir la dimensionalidad, es descubrir características importantes en los datos. Este proceso de descubrimiento en PCA se realiza a través de representaciones gráficas de los puntajes de las componentes principales. Los primeros puntajes de las componentes principales (principal component scores) pueden revelar si la mayor parte de los datos se encuentran en un subespacio lineal de \(\mathbb{R}^r\), y pueden ser utilizados para identificar valores atípicos, peculiaridades en la distribución y agrupaciones de puntos. Por su parte, los últimos puntajes de las componentes principales muestran proyecciones lineales de \(X\) con menor varianza. 

Una vez hecho nuestro PCA sobre el conjunto de datos turtles.rda nos podemos dar cuenta de lo siguiente:

* Como se puede observar en el ***Anexo 1***, en la sección ***Q1: Análisis de Componentes Principales (PCA)***, la primera componente principal explica el 97.89% de la varianza total, mientras que la segunda y tercera componentes principales explican el 1.4% y 0.7% de la varianza total, respectivamente.
* De igual manera, en esta sección se puede observar que las dos primeras componentes principales (PC1 y PC2) juntas explican el 99.29% de la variabilidad total, lo que sugiere que una reducción a dos dimensiones es adecuada para representar los datos sin perder prácticamente nada de información.
* Por otro lado, en el gráfico de dispersión, mostrado en este apartado mencionado arriba, se muestran los datos proyectados en el espacio de las dos primeras componentes principales (PC1 y PC2), coloreados por sexo (f para hembras y m para machos), y se observa que:
  1. Las hembras (f, color rojo) y los machos (m, color azul) presentan cierta separación en la proyección a lo largo de PC1, aunque no es completamente clara ni lineal.
  2. La mayoría de las diferencias entre machos y hembras parecen capturarse en PC1, lo que podría reflejar diferencias generales de tamaño o proporciones físicas.
  3. PC2 contribuye muy poco a la separación de los datos, ya que explica solo el 1.40% de la variabilidad.


\newpage

## Q2: Cópula Clayton

La cópula de Clayton resulta especialmente útil cuando existe una dependencia en la cola inferior (lower tail dependence). En general, esto implica que es adecuada para:

* Fenómenos con una propensión a experimentar eventos extremos conjuntos, particularmente en valores bajos o negativos.
* Modelar riesgos en áreas como finanzas o seguros, donde es importante capturar la probabilidad de que ocurran simultáneamente eventos adversos.
* Datos con correlación asimétrica, en los que la dependencia es más pronunciada en los valores extremos inferiores que en los superiores.
* Situaciones donde se necesita modelar la relación entre variables que tienden a comportarse de manera conjunta bajo condiciones de estrés o adversidad.

Algunos ejemplos "mas aterrizados" incluyen:

* Modelado de riesgos financieros.
* Análisis de fallos en sistemas de ingeniería.
* Estudios sobre eventos climáticos extremos.
* Evaluación de riesgos en seguros.

Por otro lado, se uso el segundo metodo de simulacion visto en clase (teorema de inversion). Citando lo visto en clase, nos apoyamos en la siguiente derivación:

\begin{align*}
C_u(v) &= \mathbb{P}[V \leq v \mid U = u] \\
&= \lim_{\Delta u \to 0} \frac{C(u + \Delta u, v) - C(u, v)}{\Delta u} \\
&= \frac{\partial C(u, v)}{\partial u}.
\end{align*}

1. Se generan  \( U \sim \text{Uniform}(0,1) \) y \( t \sim \text{Uniform}(0,1) \), independientes.

2. Se aplica la transformación \( V = C_u^{-1}(t) \), donde \( C_u^{-1} \) es la inversa parcial de la cópula respecto a \( u \).

3. Esto permite generar pares \((u, v)\) que siguen la estructura de dependencia de la cópula \(C(u, v)\), aplicando la transformación inversa a una variable aleatoria uniforme \(t\).

Como podemos observar en el **Anexo 1**, en la sección ***Q2: Cópula Clayton***, el diagrama de dispersión muestra una estructura de dependencia entre las dos variables aleatorias, con una mayor concentración de puntos en la esquina inferior izquierda, lo que es característico de la cópula de Clayton y refleja su capacidad de capturar la dependencia en la cola inferior. Por otro lado, las curvas de contorno, que muestran los niveles de probabilidad de la cópula de Clayton, evidencian esta característica mencionada. Esto se debe a una mayor densidad de probabilidad en la región de valores bajos. Una forma angular, típica de la cópula de Clayton, indica una mayor dependencia en los extremos inferiores de las variables. Además, se puede observar que hay una dispersión mayor en las regiones superiores, mostrando cierto tipo de independencia relativa para valores más altos.

\newpage

## Q3: Escalamiento multidimensional

```{r fig.align='center', fig.width=5, fig.height=4.5,  warning=FALSE, echo=FALSE}
# Matriz de distancias
distances <- matrix(c(
  0, 130, 98, 102, 103, 100, 149, 315, 91, 196, 257, 186,
  130, 0, 33, 50, 185, 73, 33, 377, 186, 94, 304, 97,
  98, 33, 0, 36, 164, 54, 58, 359, 166, 119, 287, 113,
  102, 50, 36, 0, 138, 77, 47, 330, 139, 95, 258, 146,
  103, 185, 164, 138, 0, 184, 170, 219, 45, 186, 161, 276,
  100, 73, 54, 77, 184, 0, 107, 394, 181, 168, 322, 93,
  149, 33, 58, 47, 170, 107, 0, 362, 186, 61, 289, 130,
  315, 377, 359, 330, 219, 394, 362, 0, 223, 351, 162, 467,
  91, 186, 166, 139, 45, 181, 186, 223, 0, 215, 175, 275,
  196, 94, 119, 95, 186, 168, 61, 351, 215, 0, 274, 184,
  257, 304, 287, 258, 161, 322, 289, 162, 175, 274, 0, 395,
  186, 97, 113, 146, 276, 93, 130, 467, 275, 184, 395, 0
), nrow = 12, byrow = TRUE)

# Añadir nombres de filas y columnas
cities <- c(
  "Appleton", "Beloit", "Fort Atkinson", "Madison", "Marshfield", 
  "Milwaukee", "Monroe", "Superior", "Wausau", "Dubuque", "St. Paul", "Chicago"
)
colnames(distances) <- rownames(distances) <- cities

# MDS para 1, 2, 3 dimensiones
mds_1d <- isoMDS(distances, k = 1, trace= FALSE)
mds_2d <- isoMDS(distances, k = 2, trace= FALSE)
mds_3d <- isoMDS(distances, k = 3, trace= FALSE)

#Calcular el stress para q = 1, 2, 3
stress_1d <- mds_1d$stress
stress_2d <- mds_2d$stress
stress_3d <- mds_3d$stress

# Crear un data frame para el gráfico de stress
stress_values <- data.frame(
  q = 1:3,
  stress = c(stress_1d, stress_2d, stress_3d)
)

# Configuración bidimensional con el mapa real
mds_2d_points <- data.frame(mds_2d$points)
colnames(mds_2d_points) <- c("Dim1", "Dim2")
mds_2d_points$City <- cities

# Graficar la configuración bidimensional
plot1 <- ggplot(mds_2d_points, aes(x = Dim1, y = Dim2, label = City)) +
  geom_point() +
  geom_text(vjust = -0.5) +
  labs(title = "Escalamiento Bidimensional de Ciudades",
       x = "Dimensión 1", y = "Dimensión 2") +
  theme_minimal() +
  xlim(-250, 350) +
  ylim(-100, 100) 

# Graficar el stress vs. q
plot2 <- ggplot(stress_values, aes(x = q, y = stress)) +
  geom_line() +
  geom_point() +
  labs(title = "Stress vs. Dimensión (q)", x = "Dimensión (q)", y = "Stress") +
  theme_minimal()

grid.arrange(plot1, plot2, ncol = 1)
```

Al aplicar el método de escalamiento multidimensional (MDS) a los datos de distancias entre ciudades de Wisconsin (**Anexo 1 en la sección Q3: Escalamiento multidimensional**), se tomaron las siguientes decisiones subjetivas. En primer lugar, se evaluaron soluciones en 1, 2 y 3 dimensiones, pero finalmente se optó por representar los resultados en 2 dimensiones, ya que esto facilita una interpretación visual más intuitiva. Esto es a pesar de que el análisis en 3D podría capturar una mayor variabilidad. Tenemos que recordar que las técnicas de escalamiento multidimensional se ocupan del siguiente problema: para un conjunto de N elementos, encontrar una representación de esos elementos en un espacio de pocas dimensiones, de tal forma que las proximidades (o distancias) entre los elementos en la representación "casi coincidan" con las similitudes (o distancias) originales entre los elementos (Johnson & Wichern, 2007). Cabe mencionar que se utilizaron las distancias reales entre las ciudades, las cuales se pueden ver en la sección del anexo 1 mencionada, que asegura que el modelo intente preservar estas relaciones en el espacio proyectado. Dicho esto, se ajusto datos originales a un sistema de coordenadas de baja dimensionalidad, en este caso dos, de manera que cualquier distorsión causada por esta reducción dimensional se minimice.

Como se puede observar en el gráfico en la parte superior, la configuración de dos dimensiones resulta adecuada, ya que permite visualizar de manera clara la proximidad relativa entre las ciudades. Así, ciudades cercanas en el gráfico, como Beloit y Madison, se interpretan como geográficamente próximas, mientras que aquellas más distantes, como Superior y Chicago, muestran mayores diferencias. Por otro lado, según lo observado en el gráfico de estrés, la solución de dos dimensiones captura la mayor parte de la variabilidad presente en las distancias originales, por lo que no se justifica aumentar la complejidad a tres dimensiones, a pesar de que el estrés continúa disminuyendo. En otras palabras, aunque el estrés disminuye en tres dimensiones, el cambio es marginal y no justifica la complejidad adicional.
