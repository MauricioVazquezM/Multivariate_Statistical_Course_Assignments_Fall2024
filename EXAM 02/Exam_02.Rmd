---
title: 'Segundo Examen Parcial: Otoño 2024'
author: "Mauricio Vazquez Moran (000191686)"
date: '2024-11-28'
output:
  pdf_document: default
  html_document: default
---

***Link del repositorio de GitHub: https://github.com/MauricioVazquezM/Multivariate_Statistical_Course_Assignments_Fall2024***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, fig.width=10)

# Libraries
library(MASS)  
library(ggplot2)
library(GGally)
library(mclust)
library(kableExtra)
library(FactoMineR)
library(knitr)
library(caret)
library(dplyr)
library(gridExtra)
library(ggfortify)
library(copula)
```

## Q1: Analisis de componentes principales (PCA)

Como podemos observar en el ***Anexo 2***, nuestro dataset (turtles.rda) no contiene una gran cantidad de variables. Las variables presentes en el conjunto de datos son 'length', 'width', 'height' y 'sex'. Si bien es cierto que un Análisis de Componentes Principales (PCA) se utiliza principalmente como técnica de reducción de dimensionalidad (Izenman 2013), también es cierto que esta herramienta estadística es usada para eliminar la redundancia en los datos, representándolos en términos de nuevas variables (componentes principales) que están descorrelacionadas entre sí. En pocas palabras, el objetivo principal del PCA es resumir un conjunto de variables en un número menor de variables significativas que, colectivamente, expliquen la mayor parte de la variabilidad presente en el conjunto original de datos (James et al., 2017). Bajo nuestra línea de análisis, según el ***Anexo 2***, mediante un Análisis Exploratorio de Datos (EDA), nos damos cuenta de que esta es nuestra situación. Las variables del conjunto de datos mencionado están altamente correlacionadas entre sí. 

Por otro lado, otro objetivo, además de reducir la dimensionalidad, es descubrir características importantes en los datos. Este proceso de descubrimiento en PCA se realiza a través de representaciones gráficas de los puntajes de las componentes principales. Los primeros puntajes de las componentes principales (principal component scores) pueden revelar si la mayor parte de los datos se encuentran en un subespacio lineal de \(\mathbb{R}^r\), y pueden ser utilizados para identificar valores atípicos, peculiaridades en la distribución y agrupaciones de puntos. Por su parte, los últimos puntajes de las componentes principales muestran proyecciones lineales de \(X\) con menor varianza. 

Una vez hecho nuestro PCA sobre el conjunto de datos turtles.rda nos podemos dar cuenta de lo siguiente:

* Como se puede observar en el ***Anexo 1***, en la sección ***Q1: Análisis de Componentes Principales (PCA)***, la primera componente principal explica el 97.89% de la varianza total, mientras que la segunda y tercera componentes principales explican el 1.4% y 0.7% de la varianza total, respectivamente.
* De igual manera, en esta sección se puede observar que las dos primeras componentes principales (PC1 y PC2) juntas explican el 99.29% de la variabilidad total, lo que sugiere que una reducción a dos dimensiones es adecuada para representar los datos sin perder prácticamente nada de información.
* Por otro lado, en el gráfico de dispersión, mostrado en este apartado mencionado arriba, se muestran los datos proyectados en el espacio de las dos primeras componentes principales (PC1 y PC2), coloreados por sexo (f para hembras y m para machos), y se observa que:
  1. Las hembras (f, color rojo) y los machos (m, color azul) presentan cierta separación en la proyección a lo largo de PC1, aunque no es completamente clara ni lineal.
  2. La mayoría de las diferencias entre machos y hembras parecen capturarse en PC1, lo que podría reflejar diferencias generales de tamaño o proporciones físicas.
  3. PC2 contribuye muy poco a la separación de los datos, ya que explica solo el 1.40% de la variabilidad.


\newpage

## Q2: Cópula Clayton

La cópula de Clayton resulta especialmente útil cuando existe una dependencia en la cola inferior (lower tail dependence). En general, esto implica que es adecuada para:

* Fenómenos con una propensión a experimentar eventos extremos conjuntos, particularmente en valores bajos o negativos.
* Modelar riesgos en áreas como finanzas o seguros, donde es importante capturar la probabilidad de que ocurran simultáneamente eventos adversos.
* Datos con correlación asimétrica, en los que la dependencia es más pronunciada en los valores extremos inferiores que en los superiores.
* Situaciones donde se necesita modelar la relación entre variables que tienden a comportarse de manera conjunta bajo condiciones de estrés o adversidad.

Algunos ejemplos "mas aterrizados" incluyen:

* Modelado de riesgos financieros.
* Análisis de fallos en sistemas de ingeniería.
* Estudios sobre eventos climáticos extremos.
* Evaluación de riesgos en seguros.

Por otro lado, se uso el segundo metodo de simulacion visto en clase (teorema de inversion). Este consta del siguiente procedimiento:

Citando lo visto en clase, nos apoyamos en la siguiente derivación:

\[
C_u(v) = \mathbb{P}[V \leq v \mid U = u]
\]

\[
= \lim_{\Delta u \to 0} \frac{C(u + \Delta u, v) - C(u, v)}{\Delta u}
\]

\[
= \frac{\partial C(u, v)}{\partial u}.
\]

1. Se generan  \( U \sim \text{Uniform}(0,1) \) y \( t \sim \text{Uniform}(0,1) \), independientes.

2. Se aplica la transformación \( V = C_u^{-1}(t) \), donde \( C_u^{-1} \) es la inversa parcial de la cópula respecto a \( u \).

3. Esto permite generar pares \((u, v)\) que siguen la estructura de dependencia de la cópula \(C(u, v)\), aplicando la transformación inversa a una variable aleatoria uniforme \(t\).

```{r fig.align='center', fig.width=4.5, fig.height=2.5, echo=FALSE}
# Parámetro de theta
theta <- 3  
set.seed(42)

# Cópula de Clayton
clayton_copula <- claytonCopula(param = theta, dim = 2)

# Simulando datos
n <- 1000  
simulated_data <- rCopula(n, clayton_copula)

# Pasando a un dataframe
data <- data.frame(x = simulated_data[, 1], y = simulated_data[, 2])

# Graficando
ggplot(data, aes(x = x, y = y)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_density_2d(color = "red") + 
  labs(
    title = "Simulación de Cópula Clayton",
    x = "Variable 1",
    y = "Variable 2"
  ) +
  theme_minimal()
```

\newpage

## Q3: Escalamiento multidimensional

```{r fig.align='center', fig.width=4.5, fig.height=2.5, echo=FALSE}
# Parámetros de simulación
set.seed(123) 
n <- 1000     
theta <- 3     

# Crear la cópula de Clayton
clayton_cop <- claytonCopula(param = theta, dim = 2)

# Simular muestras
samples <- rCopula(n, clayton_cop)

# Convertir a dataframe para facilitar graficación
df <- data.frame(samples)
colnames(df) <- c("X", "Y")

# Gráfico de dispersión
ggplot(df, aes(x = X, y = Y)) +
  geom_point(alpha = 0.5) +
  labs(title = "Diagrama de Dispersión - Cópula de Clayton",
       subtitle = paste("θ =", theta)) +
  theme_minimal()

# Curvas de contorno
ggplot(df, aes(x = X, y = Y)) +
  geom_density_2d_filled(contour_var = "density", alpha = 0.6) +
  labs(title = "Curvas de Contorno - Cópula de Clayton",
       subtitle = paste("θ =", theta)) +
  theme_minimal()
  
```
