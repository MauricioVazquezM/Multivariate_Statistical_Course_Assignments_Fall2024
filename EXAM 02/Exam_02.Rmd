---
title: 'Segundo Examen Parcial: Otoño 2024'
author: "Mauricio Vazquez Moran (000191686)"
date: '2024-11-28'
output:
  pdf_document: default
  html_document: default
---

***Link del repositorio de GitHub: https://github.com/MauricioVazquezM/Multivariate_Statistical_Course_Assignments_Fall2024***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, fig.width=10)

# Libraries
library(MASS)  
library(ggplot2)
library(GGally)
library(mclust)
library(kableExtra)
library(FactoMineR)
library(knitr)
library(caret)
library(dplyr)
library(gridExtra)
library(ggfortify)
library(copula)
```

## Q1: Analisis de componentes principales (PCA)

Como podemos observar en el ***Anexo 2***, nuestro dataset (turtles.rda) no contiene una gran cantidad de variables. Las variables presentes en el conjunto de datos son 'length', 'width', 'height' y 'sex'. Si bien es cierto que un Análisis de Componentes Principales (PCA) se utiliza principalmente como técnica de reducción de dimensionalidad (Izenman 2013), también es cierto que esta herramienta estadística es usada para eliminar la redundancia en los datos, representándolos en términos de nuevas variables (componentes principales) que están descorrelacionadas entre sí. En pocas palabras, el objetivo principal del PCA es resumir un conjunto de variables en un número menor de variables significativas que, colectivamente, expliquen la mayor parte de la variabilidad presente en el conjunto original de datos (James et al., 2017). Bajo nuestra línea de análisis, según el ***Anexo 2***, mediante un Análisis Exploratorio de Datos (EDA), nos damos cuenta de que esta es nuestra situación. Las variables del conjunto de datos mencionado están altamente correlacionadas entre sí. 

Por otro lado, otro objetivo, además de reducir la dimensionalidad, es descubrir características importantes en los datos. Este proceso de descubrimiento en PCA se realiza a través de representaciones gráficas de los puntajes de las componentes principales. Los primeros puntajes de las componentes principales (principal component scores) pueden revelar si la mayor parte de los datos se encuentran en un subespacio lineal de \(\mathbb{R}^r\), y pueden ser utilizados para identificar valores atípicos, peculiaridades en la distribución y agrupaciones de puntos. Por su parte, los últimos puntajes de las componentes principales muestran proyecciones lineales de \(X\) con menor varianza. 

Una vez hecho nuestro PCA sobre el conjunto de datos turtles.rda nos podemos dar cuenta de lo siguiente:

* Como se puede observar en el ***Anexo 1***, en la sección ***Q1: Análisis de Componentes Principales (PCA)***, la primera componente principal explica el 97.89% de la varianza total, mientras que la segunda y tercera componentes principales explican el 1.4% y 0.7% de la varianza total, respectivamente.
* De igual manera, en esta sección se puede observar que las dos primeras componentes principales (PC1 y PC2) juntas explican el 99.29% de la variabilidad total, lo que sugiere que una reducción a dos dimensiones es adecuada para representar los datos sin perder prácticamente nada de información.
* Por otro lado, en el gráfico de dispersión, mostrado en este apartado mencionado arriba, se muestran los datos proyectados en el espacio de las dos primeras componentes principales (PC1 y PC2), coloreados por sexo (f para hembras y m para machos), y se observa que:
  1. Las hembras (f, color rojo) y los machos (m, color azul) presentan cierta separación en la proyección a lo largo de PC1, aunque no es completamente clara ni lineal.
  2. La mayoría de las diferencias entre machos y hembras parecen capturarse en PC1, lo que podría reflejar diferencias generales de tamaño o proporciones físicas.
  3. PC2 contribuye muy poco a la separación de los datos, ya que explica solo el 1.40% de la variabilidad.


\newpage

## Q2: Cópula Clayton

La cópula de Clayton resulta especialmente útil cuando existe una dependencia en la cola inferior (lower tail dependence). En general, esto implica que es adecuada para:

* Fenómenos con una propensión a experimentar eventos extremos conjuntos, particularmente en valores bajos o negativos.
* Modelar riesgos en áreas como finanzas o seguros, donde es importante capturar la probabilidad de que ocurran simultáneamente eventos adversos.
* Datos con correlación asimétrica, en los que la dependencia es más pronunciada en los valores extremos inferiores que en los superiores.
* Situaciones donde se necesita modelar la relación entre variables que tienden a comportarse de manera conjunta bajo condiciones de estrés o adversidad.

Algunos ejemplos "mas aterrizados" incluyen:

* Modelado de riesgos financieros.
* Análisis de fallos en sistemas de ingeniería.
* Estudios sobre eventos climáticos extremos.
* Evaluación de riesgos en seguros.

Por otro lado, se uso el segundo metodo de simulacion visto en clase (teorema de inversion). Este consta del siguiente procedimiento:

Citando lo visto en clase, nos apoyamos en la siguiente derivación:

Aquí está el código en R Markdown para alinear las ecuaciones:

\begin{align*}
C_u(v) &= \mathbb{P}[V \leq v \mid U = u] \\
&= \lim_{\Delta u \to 0} \frac{C(u + \Delta u, v) - C(u, v)}{\Delta u} \\
&= \frac{\partial C(u, v)}{\partial u}.
\end{align*}


1. Se generan  \( U \sim \text{Uniform}(0,1) \) y \( t \sim \text{Uniform}(0,1) \), independientes.

2. Se aplica la transformación \( V = C_u^{-1}(t) \), donde \( C_u^{-1} \) es la inversa parcial de la cópula respecto a \( u \).

3. Esto permite generar pares \((u, v)\) que siguen la estructura de dependencia de la cópula \(C(u, v)\), aplicando la transformación inversa a una variable aleatoria uniforme \(t\).

Como podemos observar en el **Anexo 1**, en la sección ***Q2: Cópula Clayton***, el diagrama de dispersión muestra una estructura de dependencia entre las dos variables aleatorias, con una mayor concentración de puntos en la esquina inferior izquierda, lo que es característico de la cópula de Clayton y refleja su capacidad de capturar la dependencia en la cola inferior. Por otro lado, las curvas de contorno, que muestran los niveles de probabilidad de la cópula de Clayton, evidencian esta característica mencionada. Esto se debe a una mayor densidad de probabilidad en la región de valores bajos. Una forma angular, típica de la cópula de Clayton, indica una mayor dependencia en los extremos inferiores de las variables. Además, se puede observar que hay una dispersión mayor en las regiones superiores, mostrando cierto tipo de independencia relativa para valores más altos.

\newpage

## Q3: Escalamiento multidimensional

```{r fig.align='center', fig.width=4.5, fig.height=2.5, echo=FALSE}

  
```
