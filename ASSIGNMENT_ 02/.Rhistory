qsaux <- c(qsaux, qs)
}
## Getting the min amplitude interval
min_amplitude <- l.min()
## Getting index of the min amplitude interval
min_index <- which.min(l)
## Getting the limits of the interval
limit_inf_f <- qiaux[min_index]
limit_upp_f <- qsaux[min_index]
## Storing on the table
chi_table[[length(chi_table) +1]] <- list(Degrees_of_freedom = n, CI = ci, qi = limit_inf_f, qs = limit_upp_f, w = min_amplitude)
}
}
return(chi_table)
}
# Using our function
chi_ta2 <- chi_table_function_MINw(degreesF_vector, confidence_intervals)
## Creating function for easy usage(same weight tails)
chi_table_function_MINw <- function(n_vector, conf_int_vector){
## Defining list to store outputs
chi_table <- list()
## Definins auxiliars list
l <- numeric(0)
qiaux <- numeric(0)
qsaux <- numeric(0)
## Iterating over each sigma value
for (n in n_vector) {
## Iterating over each confidence interval value we have
for (ci in conf_int_vector){
## Defining alpha
alpha <- 1 - ci
## Defining a limit to iterate
limit <- qchisq(alpha/2, n)
## Defining my random numbers
numbers <- runif(100, min=0, max=limit)
## Iterating
for (num in numbers){
## Defining auxiliar variable
a <- pchisq(num, n)
## Defining qsuperior
qs <- qchisq(1 +a -alpha, n)
## Calculating limits
limit_inf <- num
limit_upp <- qs
## Calculating w value (Difference between upper limit and lower limit)
w <- limit_upp - limit_inf
## Storing w data
l <- c(l, w)
## Storing auxiliar data
qiaux <- c(qiaux, num)
qsaux <- c(qsaux, qs)
}
## Getting the min amplitude interval
min_amplitude <- min(l)
## Getting index of the min amplitude interval
min_index <- which.min(l)
## Getting the limits of the interval
limit_inf_f <- qiaux[min_index]
limit_upp_f <- qsaux[min_index]
## Storing on the table
chi_table[[length(chi_table) +1]] <- list(Degrees_of_freedom = n, CI = ci, qi = limit_inf_f, qs = limit_upp_f, w = min_amplitude)
}
}
return(chi_table)
}
# Using our function
chi_ta2 <- chi_table_function_MINw(degreesF_vector, confidence_intervals)
# Visualizing results
print(chi_ta2)
View(chi_ta2)
## Creating function for easy usage(same weight tails)
chi_table_function_MINw <- function(n_vector, conf_int_vector){
## Defining list to store outputs
chi_table <- list()
## Iterating over each sigma value
for (n in n_vector) {
## Iterating over each confidence interval value we have
for (ci in conf_int_vector){
## Defining auxiliars vectors
l <- numeric(0)
qiaux <- numeric(0)
qsaux <- numeric(0)
## Defining alpha
alpha <- 1 - ci
## Defining a limit to iterate
limit <- qchisq(alpha/2, n)
## Defining my random numbers
numbers <- runif(100, min=0, max=limit)
## Iterating
for (num in numbers){
## Defining auxiliar variable
a <- pchisq(num, n)
## Defining qsuperior
qf <- qchisq(1 + a - alpha, n)
## Calculating limits
limit_inf <- num
limit_upp <- qf
## Calculating w value (Difference between upper limit and lower limit)
w <- limit_upp - limit_inf
## Storing w data
l <- c(l, w)
## Storing auxiliar data
qiaux <- c(qiaux, num)
qsaux <- c(qsaux, qs)
}
## Getting the min amplitude interval
min_amplitude <- min(l)
## Getting index of the min amplitude interval
min_index <- which.min(l)
## Getting the limits of the interval
limit_inf_f <- qiaux[min_index]
limit_upp_f <- qsaux[min_index]
## Storing on the table
chi_table[[length(chi_table) +1]] <- list(Degrees_of_freedom = n, CI = ci, qi = limit_inf_f, qs = limit_upp_f, w = min_amplitude)
}
}
return(chi_table)
}
# Using our function
chi_ta2 <- chi_table_function_MINw(degreesF_vector, confidence_intervals)
## Creating function for easy usage(same weight tails)
chi_table_function_MINw <- function(n_vector, conf_int_vector){
## Defining list to store outputs
chi_table <- list()
## Iterating over each sigma value
for (n in n_vector) {
## Iterating over each confidence interval value we have
for (ci in conf_int_vector){
## Defining auxiliars vectors
l <- numeric(0)
qiaux <- numeric(0)
qsaux <- numeric(0)
## Defining alpha
alpha <- 1 - ci
## Defining a limit to iterate
limit <- qchisq(alpha/2, n)
## Defining my random numbers
numbers <- runif(100, min=0, max=limit)
## Iterating
for (num in numbers){
## Defining auxiliar variable
a <- pchisq(num, n)
## Defining qsuperior
qf <- qchisq(1 + a - alpha, n)
## Calculating limits
limit_inf <- num
limit_upp <- qf
## Calculating w value (Difference between upper limit and lower limit)
w <- limit_upp - limit_inf
## Storing w data
l <- c(l, w)
## Storing auxiliar data
qiaux <- c(qiaux, num)
qsaux <- c(qsaux, qf)
}
## Getting the min amplitude interval
min_amplitude <- min(l)
## Getting index of the min amplitude interval
min_index <- which.min(l)
## Getting the limits of the interval
limit_inf_f <- qiaux[min_index]
limit_upp_f <- qsaux[min_index]
## Storing on the table
chi_table[[length(chi_table) +1]] <- list(Degrees_of_freedom = n, CI = ci, qi = limit_inf_f, qs = limit_upp_f, w = min_amplitude)
}
}
return(chi_table)
}
# Using our function
chi_ta2 <- chi_table_function_MINw(degreesF_vector, confidence_intervals)
# Visualizing results
print(chi_ta2)
View(chi_ta2)
## B.2
## Creating function for easy usage(same weight tails)
chi_table_function_MINw <- function(n_vector, conf_int_vector){
## Defining list to store outputs
chi_table <- list()
## Iterating over each sigma value
for (n in n_vector) {
## Iterating over each confidence interval value we have
for (ci in conf_int_vector){
## Defining auxiliars vectors
l <- numeric(0)
qiaux <- numeric(0)
qsaux <- numeric(0)
## Defining alpha
alpha <- 1 - ci
## Defining a limit to iterate
limit <- qchisq(alpha/2, n)
## Defining my random numbers
numbers <- runif(100, min=0, max=limit)
## Iterating
for (num in numbers){
## Defining auxiliar variable
a <- pchisq(num, n)
## Defining qsuperior
qf <- qchisq(1 + a - alpha, n)
## Calculating limits
limit_inf <- num
limit_upp <- qf
## Calculating w value (Difference between upper limit and lower limit)
w <- limit_upp - limit_inf
## Storing w data
l <- c(l, w)
## Storing auxiliar data
qiaux <- c(qiaux, num)
qsaux <- c(qsaux, qf)
}
## Getting the min amplitude interval
min_amplitude <- min(l)
## Getting index of the min amplitude interval
min_index <- which.min(l)
## Getting the limits of the interval
limit_inf_f <- qiaux[min_index]
limit_upp_f <- qsaux[min_index]
## Storing on the table
chi_table[[length(chi_table) +1]] <- list(Degrees_of_freedom = n, CI = ci, qi = limit_inf_f, qs = limit_upp_f, w = min_amplitude)
}
}
return(chi_table)
}
# Using our function
chi_ta2 <- chi_table_function_MINw(degreesF_vector, confidence_intervals)
# Visualizing results
print(chi_ta2)
View(chi_ta2)
## Modern Multivariate Statistical Techniques (Izenman) Ex. 8.2
library(MASS)
library(ggplot2)
library(GGally)
install.packages("GGally")
## Modern Multivariate Statistical Techniques (Izenman) Ex. 8.5
# Loading dataset
load("C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_01/diabetes.rda")
# Parsing to dataframe
diabetes_df <- as.data.frame(diabetes)
# Checking head dataset
head(diabetes_df)
# Scatterplot matrix of all five variables, colored by class
ggpairs(diabetes_df, columns = 1:5, aes(color = class)) +
labs(title = "Scatterplot Matrix of Diabetes Data")
data(eurodist)
eurodist
install.packages("knitr")
install.packages("kableExtra")
install.packages('tinytex')
tinytex::install_tinytex()
tinytex::pdflatex('Assignment01.tex')
library(tidyverse)
library(palmerpenguins)
set.seed(9450)
penguin_df<-
palmerpenguins::penguins %>%
na.omit()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
lin_reg <- lm(bill_depth_mm ~ bill_length_mm, data=penguin_df)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
summary(lin_reg)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm,
color=species)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
chin<-
penguin_df %>%
filter(species == "Chinstrap")
adelie<-
penguin_df %>%
filter(species == "Adelie")
gentoo<-
penguin_df %>%
filter(species == "Gentoo")
lm_chin<- lm(data=chin, bill_depth_mm ~ bill_length_mm)
lm_adelie<- lm(data=adelie, bill_depth_mm ~ bill_length_mm)
lm_gentoo<- lm(data=gentoo, bill_depth_mm ~ bill_length_mm)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm,
color=species)) +
geom_point() +
geom_abline(slope = lm_chin$coefficients[[2]],
intercept = lm_chin$coefficients[[1]],
color="black") +
geom_abline(slope = lm_adelie$coefficients[[2]],
intercept = lm_adelie$coefficients[[1]],
color="black") +
geom_abline(slope = lm_gentoo$coefficients[[2]],
intercept = lm_gentoo$coefficients[[1]],
color="black") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
library(tidyverse)
library(palmerpenguins)
set.seed(9450)
penguin_df<-
palmerpenguins::penguins %>%
na.omit()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
lin_reg <- lm(bill_depth_mm ~ bill_length_mm, data=penguin_df)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
# Crear el histograma
histograma <- ggplot(data, aes(x = performance_idx)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Histograma de performance index", x = "performance index", y = "Frecuencia") +
theme_minimal()
# Leer el archivo
data <- read.csv("C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS LINEALES/REPOSITORIO/LINEAR_METHODS_ASSIGNMENTS_FALL2024/PROJECT/Student_Performance.csv")
# Ver las primeras filas del DataFrame
head(data)
# Nombres originales de las columnas
colnames(data) <- c("Hours.Studied", "Previous.Scores", "Extracurricular.Activities",
"Sleep.Hours", "Sample.Question.Papers.Practiced", "Performance.Index")
# Renombrar las columnas (estandarizando los nombres de las columnas)
colnames(data) <- tolower(colnames(data))
colnames(data) <- gsub("\\.", "_", colnames(data))
# Acortar los nombres de las columnas
colnames(data) <- c("hrs_studied", "prev_scores", "xtr_activities",
"sleep_hrs", "sample_questions", "performance_idx")
# Verificar los nuevos nombres
print(colnames(data))
# Funcion de analisis univariado
univar_analisis <- function(data) {
results <- list()
for (feature in colnames(data)) {
data_type <- class(data[[feature]])[1]
total <- nrow(data)
nan_count <- sum(is.na(data[[feature]]))
no_missings <- total - nan_count
pct_missings <- nan_count / total
if (is.numeric(data[[feature]])) {
promedio <- round(mean(data[[feature]], na.rm = TRUE),2)
desv_estandar <- round(sd(data[[feature]], na.rm = TRUE),2)
varianza <- round(var(data[[feature]], na.rm = TRUE),2)
minimo <- min(data[[feature]], na.rm = TRUE)
p10 <- quantile(data[[feature]], 0.10, na.rm = TRUE)
q1 <- quantile(data[[feature]], 0.25, na.rm = TRUE)
mediana <- quantile(data[[feature]], 0.50, na.rm = TRUE)
q3 <- quantile(data[[feature]], 0.75, na.rm = TRUE)
p90 <- quantile(data[[feature]], 0.90, na.rm = TRUE)
p95 <- quantile(data[[feature]], 0.95, na.rm = TRUE)
p99 <- quantile(data[[feature]], 0.99, na.rm = TRUE)
maximo <- max(data[[feature]], na.rm = TRUE)
inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] > 0)
neg_inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] < 0)
} else {
promedio <- NA
desv_estandar <- NA
varianza <- NA
minimo <- NA
p1 <- NA
p5 <- NA
p10 <- NA
q1 <- NA
mediana <- NA
q3 <- NA
p90 <- NA
p95 <- NA
p99 <- NA
maximo <- NA
inf_count <- 0
neg_inf_count <- 0
}
results[[length(results) + 1]] <- list(
Variable = feature,
Total = total,
No_Missings = no_missings,
Missings = nan_count,
Pct_Missings = pct_missings,
Promedio = promedio,
Desv_std = desv_estandar,
Varianza = varianza,
Minimo = minimo,
p10 = p10,
q1 = q1,
Mediana = mediana,
q3 = q3,
p90 = p90,
p95 = p95,
p99 = p99,
Maximo = maximo
)
}
result_df <- do.call(rbind, lapply(results, as.data.frame))
rownames(result_df) <- NULL
return(result_df)
}
# Ejecutar la función de análisis univariante
resultados <- univar_analisis(data)
# Crear el histograma
histograma <- ggplot(data, aes(x = performance_idx)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Histograma de performance index", x = "performance index", y = "Frecuencia") +
theme_minimal()
head(data)
hea(data)
head(data)
View(data)
# Analisis de Linealidad y Homocedasticidad
modelo <- lm(data = data, formula = performance_idx ~ prev_score)
# Analisis de Linealidad y Homocedasticidad
modelo <- lm(data = data, formula = performance_idx ~ prev_scores)
ggplot(data = data) +
aes(x = modelo$fitted.values, y = modelo$residuals) +
geom_vline(xintercept = median(modelo$fitted.values)) +
geom_point() +
theme_classic()
# Leyendo archivo
file_path <- "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS LINEALES/REPOSITORIO/LINEAR_METHODS_ASSIGNMENTS_FALL2024/ASSIGNMENT 2/datos_ejercicio_2_tarea_3.txt"
# Leyendo el archivo con el delimitador '|'
datos <- read.delim(file_path, sep = "|")
head(datos)
# Ajustar el modelo de regresión lineal
modelo <- lm(BMI18 ~ Sexo + WT2 + HT2 + WT9 + HT9 + LG9 + ST9 + WT18 + HT18 + LG18 + ST18 + Soma, data = datos)
# Ajustar el modelo de regresión lineal
modelo <- lm(BMI18 ~ Sex + WT2 + HT2 + WT9 + HT9 + LG9 + ST9 + WT18 + HT18 + LG18 + ST18 + Soma, data = datos)
# Mostrar los coeficientes estimados
summary(modelo)$coefficients
# Obtener el resumen del modelo
summary_model <- summary(modelo)
# Extraer R2 y R2 ajustado
r_squared <- summary_model$r.squared
adj_r_squared <- summary_model$adj.r.squared
# Ver la significancia de los coeficientes
coef_significance <- summary_model$coefficients
# Valores para la prediccion
nuevo_sujeto <- data.frame(
Sex = NA,
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71,
WT18 = NA,
HT18 = NA,
LG18 = NA,
ST18 = NA,
Soma = NA
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Valores para la prediccion
nuevo_sujeto <- data.frame(
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Valores para la prediccion
nuevo_sujeto <- data.frame(
Sex = 0,
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71,
WT18 = 0,
HT18 = 0,
LG18 = 0,
ST18 = 0,
Soma = 0
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Mostrar la predicción y el intervalo de confianza
prediccion
setwd("~/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSOGNMENT 02")
