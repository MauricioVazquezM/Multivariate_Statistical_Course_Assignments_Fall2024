chi_ta2 <- chi_table_function_MINw(degreesF_vector, confidence_intervals)
# Visualizing results
print(chi_ta2)
View(chi_ta2)
## Modern Multivariate Statistical Techniques (Izenman) Ex. 8.2
library(MASS)
library(ggplot2)
library(GGally)
install.packages("GGally")
## Modern Multivariate Statistical Techniques (Izenman) Ex. 8.5
# Loading dataset
load("C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_01/diabetes.rda")
# Parsing to dataframe
diabetes_df <- as.data.frame(diabetes)
# Checking head dataset
head(diabetes_df)
# Scatterplot matrix of all five variables, colored by class
ggpairs(diabetes_df, columns = 1:5, aes(color = class)) +
labs(title = "Scatterplot Matrix of Diabetes Data")
data(eurodist)
eurodist
install.packages("knitr")
install.packages("kableExtra")
install.packages('tinytex')
tinytex::install_tinytex()
tinytex::pdflatex('Assignment01.tex')
library(tidyverse)
library(palmerpenguins)
set.seed(9450)
penguin_df<-
palmerpenguins::penguins %>%
na.omit()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
lin_reg <- lm(bill_depth_mm ~ bill_length_mm, data=penguin_df)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
summary(lin_reg)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm,
color=species)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
chin<-
penguin_df %>%
filter(species == "Chinstrap")
adelie<-
penguin_df %>%
filter(species == "Adelie")
gentoo<-
penguin_df %>%
filter(species == "Gentoo")
lm_chin<- lm(data=chin, bill_depth_mm ~ bill_length_mm)
lm_adelie<- lm(data=adelie, bill_depth_mm ~ bill_length_mm)
lm_gentoo<- lm(data=gentoo, bill_depth_mm ~ bill_length_mm)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm,
color=species)) +
geom_point() +
geom_abline(slope = lm_chin$coefficients[[2]],
intercept = lm_chin$coefficients[[1]],
color="black") +
geom_abline(slope = lm_adelie$coefficients[[2]],
intercept = lm_adelie$coefficients[[1]],
color="black") +
geom_abline(slope = lm_gentoo$coefficients[[2]],
intercept = lm_gentoo$coefficients[[1]],
color="black") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
library(tidyverse)
library(palmerpenguins)
set.seed(9450)
penguin_df<-
palmerpenguins::penguins %>%
na.omit()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
lin_reg <- lm(bill_depth_mm ~ bill_length_mm, data=penguin_df)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
geom_abline(slope = lin_reg$coefficients[[2]],
intercept = lin_reg$coefficients[[1]],
color="red") +
labs(x="Length", y="Depth",
title="Regression of Depth as a function of Length") +
theme_classic()
# Crear el histograma
histograma <- ggplot(data, aes(x = performance_idx)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Histograma de performance index", x = "performance index", y = "Frecuencia") +
theme_minimal()
# Leer el archivo
data <- read.csv("C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS LINEALES/REPOSITORIO/LINEAR_METHODS_ASSIGNMENTS_FALL2024/PROJECT/Student_Performance.csv")
# Ver las primeras filas del DataFrame
head(data)
# Nombres originales de las columnas
colnames(data) <- c("Hours.Studied", "Previous.Scores", "Extracurricular.Activities",
"Sleep.Hours", "Sample.Question.Papers.Practiced", "Performance.Index")
# Renombrar las columnas (estandarizando los nombres de las columnas)
colnames(data) <- tolower(colnames(data))
colnames(data) <- gsub("\\.", "_", colnames(data))
# Acortar los nombres de las columnas
colnames(data) <- c("hrs_studied", "prev_scores", "xtr_activities",
"sleep_hrs", "sample_questions", "performance_idx")
# Verificar los nuevos nombres
print(colnames(data))
# Funcion de analisis univariado
univar_analisis <- function(data) {
results <- list()
for (feature in colnames(data)) {
data_type <- class(data[[feature]])[1]
total <- nrow(data)
nan_count <- sum(is.na(data[[feature]]))
no_missings <- total - nan_count
pct_missings <- nan_count / total
if (is.numeric(data[[feature]])) {
promedio <- round(mean(data[[feature]], na.rm = TRUE),2)
desv_estandar <- round(sd(data[[feature]], na.rm = TRUE),2)
varianza <- round(var(data[[feature]], na.rm = TRUE),2)
minimo <- min(data[[feature]], na.rm = TRUE)
p10 <- quantile(data[[feature]], 0.10, na.rm = TRUE)
q1 <- quantile(data[[feature]], 0.25, na.rm = TRUE)
mediana <- quantile(data[[feature]], 0.50, na.rm = TRUE)
q3 <- quantile(data[[feature]], 0.75, na.rm = TRUE)
p90 <- quantile(data[[feature]], 0.90, na.rm = TRUE)
p95 <- quantile(data[[feature]], 0.95, na.rm = TRUE)
p99 <- quantile(data[[feature]], 0.99, na.rm = TRUE)
maximo <- max(data[[feature]], na.rm = TRUE)
inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] > 0)
neg_inf_count <- sum(is.infinite(data[[feature]]) & data[[feature]] < 0)
} else {
promedio <- NA
desv_estandar <- NA
varianza <- NA
minimo <- NA
p1 <- NA
p5 <- NA
p10 <- NA
q1 <- NA
mediana <- NA
q3 <- NA
p90 <- NA
p95 <- NA
p99 <- NA
maximo <- NA
inf_count <- 0
neg_inf_count <- 0
}
results[[length(results) + 1]] <- list(
Variable = feature,
Total = total,
No_Missings = no_missings,
Missings = nan_count,
Pct_Missings = pct_missings,
Promedio = promedio,
Desv_std = desv_estandar,
Varianza = varianza,
Minimo = minimo,
p10 = p10,
q1 = q1,
Mediana = mediana,
q3 = q3,
p90 = p90,
p95 = p95,
p99 = p99,
Maximo = maximo
)
}
result_df <- do.call(rbind, lapply(results, as.data.frame))
rownames(result_df) <- NULL
return(result_df)
}
# Ejecutar la función de análisis univariante
resultados <- univar_analisis(data)
# Crear el histograma
histograma <- ggplot(data, aes(x = performance_idx)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Histograma de performance index", x = "performance index", y = "Frecuencia") +
theme_minimal()
head(data)
hea(data)
head(data)
View(data)
# Analisis de Linealidad y Homocedasticidad
modelo <- lm(data = data, formula = performance_idx ~ prev_score)
# Analisis de Linealidad y Homocedasticidad
modelo <- lm(data = data, formula = performance_idx ~ prev_scores)
ggplot(data = data) +
aes(x = modelo$fitted.values, y = modelo$residuals) +
geom_vline(xintercept = median(modelo$fitted.values)) +
geom_point() +
theme_classic()
# Leyendo archivo
file_path <- "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS LINEALES/REPOSITORIO/LINEAR_METHODS_ASSIGNMENTS_FALL2024/ASSIGNMENT 2/datos_ejercicio_2_tarea_3.txt"
# Leyendo el archivo con el delimitador '|'
datos <- read.delim(file_path, sep = "|")
head(datos)
# Ajustar el modelo de regresión lineal
modelo <- lm(BMI18 ~ Sexo + WT2 + HT2 + WT9 + HT9 + LG9 + ST9 + WT18 + HT18 + LG18 + ST18 + Soma, data = datos)
# Ajustar el modelo de regresión lineal
modelo <- lm(BMI18 ~ Sex + WT2 + HT2 + WT9 + HT9 + LG9 + ST9 + WT18 + HT18 + LG18 + ST18 + Soma, data = datos)
# Mostrar los coeficientes estimados
summary(modelo)$coefficients
# Obtener el resumen del modelo
summary_model <- summary(modelo)
# Extraer R2 y R2 ajustado
r_squared <- summary_model$r.squared
adj_r_squared <- summary_model$adj.r.squared
# Ver la significancia de los coeficientes
coef_significance <- summary_model$coefficients
# Valores para la prediccion
nuevo_sujeto <- data.frame(
Sex = NA,
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71,
WT18 = NA,
HT18 = NA,
LG18 = NA,
ST18 = NA,
Soma = NA
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Valores para la prediccion
nuevo_sujeto <- data.frame(
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Valores para la prediccion
nuevo_sujeto <- data.frame(
Sex = 0,
WT2 = 15,
HT2 = 90,
WT9 = 35,
HT9 = 142,
LG9 = 32,
ST9 = 71,
WT18 = 0,
HT18 = 0,
LG18 = 0,
ST18 = 0,
Soma = 0
)
# Predicción puntual y intervalo de confianza al 99%
prediccion <- predict(modelo, newdata = nuevo_sujeto, interval = "confidence", level = 0.99)
# Mostrar la predicción y el intervalo de confianza
prediccion
setwd("~/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_02")
# ---------------------------- PREGUNTA 3.3 ---------------------------- #
install.packages("copula")
install.packages("ggplot2")
install.packages("GGally")
# Leyendo datos
wine_data <- read.csv( "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_02/winequality-red.csv")
# Cargar y transformar el conjunto de datos wine_data a pseudo-observaciones
wine_data_pseudo <- as.data.frame(sapply(wine_data, function(x) rank(x) / (length(x) + 1)))
# Generar un gráfico de pares con contornos de pseudo-cópula
ggpairs(
wine_data_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "green")),
diag = list(continuous = wrap("barDiag", color = "grey")),
upper = list(continuous = wrap("cor", size = 5, color = "red"))
)
# ---------------------------- PREGUNTA 3.3 ---------------------------- #
# Librerias necesarias para esta pregunta
library(copula)
library(ggplot2)
library(GGally)
# Leyendo datos
wine_data <- read.csv( "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_02/winequality-red.csv")
# Cargar y transformar el conjunto de datos wine_data a pseudo-observaciones
wine_data_pseudo <- as.data.frame(sapply(wine_data, function(x) rank(x) / (length(x) + 1)))
# Generar un gráfico de pares con contornos de pseudo-cópula
ggpairs(
wine_data_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "green")),
diag = list(continuous = wrap("barDiag", color = "grey")),
upper = list(continuous = wrap("cor", size = 5, color = "red"))
)
View(wine_data)
View(wine_data_pseudo)
View(wine_data_pseudo)
# Leyendo datos
wine_data <- read.csv( "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_02/winequality-red.csv")
# Cargar y transformar el conjunto de datos wine_data a pseudo-observaciones
wine_data_pseudo <- as.data.frame(sapply(wine_data, function(x) rank(x) / (length(x) + 1)))
# Generar un gráfico de pares con contornos de pseudo-cópula
ggpairs(
wine_data_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "green")),
diag = list(continuous = wrap("barDiag", color = "grey")),
upper = list(continuous = wrap("cor", size = 5, color = "red"))
)
# Generar un gráfico de pares con contornos de pseudo-cópula
ggpairs(
wine_data_pseudo,
lower = list(continuous = "density"),      # Gráficos de densidad en la parte inferior
diag = list(continuous = "barDiag"),       # Histograma en la diagonal
upper = list(continuous = "cor")           # Correlación en la parte superior
)
# Generar un gráfico de pares con contornos de pseudo-cópula
ggpairs(
wine_data_pseudo,
lower = list(continuous = "density"),      # Gráficos de densidad en la parte inferior
diag = list(continuous = "barDiag"),       # Histograma en la diagonal
upper = list(continuous = "cor")           # Correlación en la parte superior
)
View(wine_data)
# Leyendo datos
wine_data_path <- "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_02/winequality-red.csv"
wine_data <- read.table(wine_data_path, header = TRUE, sep = ";", quote = "\"")
# Leyendo datos
wine_data_path <- "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_02/winequality-red.txt"
wine_data <- read.table(wine_data_path, header = TRUE, sep = ";", quote = "\"")
# Cargar y transformar el conjunto de datos wine_data a pseudo-observaciones
wine_data_pseudo <- as.data.frame(sapply(wine_data, function(x) rank(x) / (length(x) + 1)))
# Generar un gráfico de pares con contornos de pseudo-cópula
ggpairs(
wine_data_pseudo,
lower = list(continuous = "density"),
diag = list(continuous = "barDiag"),
upper = list(continuous = "cor")
)
# Generar un gráfico de pares con contornos de pseudo-cópula
ggpairs(
wine_data_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "green")),
diag = list(continuous = wrap("barDiag", color = "grey")),
upper = list(continuous = wrap("cor", size = 5, color = "red"))
)
View(wine_data_pseudo)
# ---------------------------- PREGUNTA 3.4 ---------------------------- #
# Data uranium
data_uranium <- data(uranium, package = 'copula')
View(uranium)
# ---------------------------- PREGUNTA 3.4 ---------------------------- #
# Cargando data uranium
data(uranium, package = 'copula')
# Paso 1: Transformar los datos al "copula scale" usando distribuciones empíricas marginales
uranium_pseudo <- as.data.frame(sapply(uranium, function(x) rank(x) / (length(x) + 1)))
# Paso 2: Crear gráficos de contornos normalizados para todas las parejas de variables
ggpairs(
uranium_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "blue")),
diag = list(continuous = wrap("barDiag", color = "grey", bins = 10)),
upper = list(continuous = wrap("cor", size = 3, color = "red"))
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0)
)
# Paso 2: Crear gráficos de contornos normalizados para todas las parejas de variables
ggpairs(
uranium_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "blue")),
diag = list(continuous = wrap("barDiag", color = "grey", bins = 10)),
upper = list(continuous = wrap("scatter", size = 3, color = "red"))
) +
theme(
axis.text.x = element_text(angle = 90, hjust = 1),
axis.text.y = element_text(angle = 0)
)
# ---------------------------- PREGUNTA 3.4 ---------------------------- #
# Cargando data uranium
data(uranium, package = 'copula')
# Paso 1: Transformar los datos al "copula scale" usando distribuciones empíricas marginales
uranium_pseudo <- as.data.frame(sapply(uranium, function(x) rank(x) / (length(x) + 1)))
# Paso 2: Crear gráficos de contornos normalizados para todas las parejas de variables
ggpairs(
uranium_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "blue")),
diag = list(continuous = wrap("barDiag", color = "grey", bins = 10)),
upper = list(continuous = wrap("points", size = 3, color = "red"))
) +
theme(
axis.text.x = element_text(angle = 90, hjust = 1),
axis.text.y = element_text(angle = 0)
)
# Paso 2: Crear gráficos de contornos normalizados para todas las parejas de variables
ggpairs(
uranium_pseudo,
lower = list(continuous = wrap("density", alpha = 0.7, color = "blue")),
diag = list(continuous = wrap("barDiag", color = "grey", bins = 10)),
upper = list(continuous = wrap("cor", size = 3, color = "red"))
) +
theme(
axis.text.x = element_text(angle = 90, hjust = 1),
axis.text.y = element_text(angle = 0)
)
# 2. Selección de un par de variables para ajustar y comparar (e.g., Cs y Sc)
u <- uranium_pseudo$Cs
v <- uranium_pseudo$Sc
# 3. Ajuste de diferentes cópulas a las pseudo-observaciones
# Ajuste de una cópula Gaussiana
fit_gauss <- fitCopula(normalCopula(dim = 2), cbind(u, v), method = "ml")
# Ajuste de una cópula t-Student
fit_t <- fitCopula(tCopula(dim = 2), cbind(u, v), method = "ml")
# Ajuste de una cópula Clayton
fit_clayton <- fitCopula(claytonCopula(dim = 2), cbind(u, v), method = "ml")
# Ajuste de una cópula Gumbel
fit_gumbel <- fitCopula(gumbelCopula(dim = 2), cbind(u, v), method = "ml")
# 4. Generación de contornos empíricos
empirical_plot <- ggplot(data = data.frame(u, v), aes(x = u, y = v)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon") +
labs(title = "Empirical Contour Plot") +
theme_minimal()
# 5. Generación de contornos ajustados para cada cópula
# Función auxiliar para generar contornos ajustados
generate_contour_plot <- function(copula_model, title) {
simulated_data <- rCopula(1000, copula_model@copula)
ggplot(data = data.frame(simulated_data), aes(x = X1, y = X2)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon") +
labs(title = title) +
theme_minimal()
}
# Gráficos de contornos ajustados
gauss_plot <- generate_contour_plot(fit_gauss, "Gaussian Copula Contour Plot")
t_plot <- generate_contour_plot(fit_t, "t-Copula Contour Plot")
clayton_plot <- generate_contour_plot(fit_clayton, "Clayton Copula Contour Plot")
gumbel_plot <- generate_contour_plot(fit_gumbel, "Gumbel Copula Contour Plot")
# Mostrar todos los gráficos
library(gridExtra)
grid.arrange(empirical_plot, gauss_plot, t_plot, clayton_plot, gumbel_plot, ncol = 2)
grid.arrange(empirical_plot, gauss_plot, t_plot, clayton_plot, gumbel_plot, ncol = 5)
View(wine_data)
head(wine_data_pseudo)
View(wine_data_pseudo)
wine_data_pseudo.columns
colnames(wine_data_pseudo)
colname(uranium_pseudo)
colnames(uranium_pseudo)
# ---------------------------- PREGUNTA 3.4 ---------------------------- #
# Cargando data uranium
data(uranium, package = 'copula')
# Transformar los datos al "copula scale" usando distribuciones empíricas marginales
uranium_pseudo <- as.data.frame(sapply(uranium, function(x) rank(x) / (length(x) + 1)))
# Quedandonos con 3 columnas
uranium_data_subset <- uranium_data_pseudo[, c("Co", "K","Cs")]
# Quedandonos con 3 columnas
uranium_data_subset <- uranium_pseudo[, c("Co", "K","Cs")]
# Grafico copula
ggpairs(
uranium_data_subset,
lower = list(continuous = wrap("density", alpha = 0.7, color = "blue")),
diag = list(continuous = wrap("barDiag", color = "grey", bins = 10)),
upper = list(continuous = wrap("cor", size = 3, color = "red"))
) +
theme(
axis.text.x = element_text(angle = 90, hjust = 1),
axis.text.y = element_text(angle = 0)
)
# Leyendo datos
wine_data_path <- "C:/Users/mauva/OneDrive/Documents/ITAM/9no Semestre/METODOS MULTIVARIADOS/REPOSITORIO/Multivariate_Statistical_Course_Assignments_Fall2024/ASSIGNMENT_02/winequality-red.txt"
wine_data <- read.table(wine_data_path, header = TRUE, sep = ";", quote = "\"")
# Cargar y transformar el conjunto de datos wine_data a pseudo-observaciones
wine_data_pseudo <- as.data.frame(sapply(wine_data, function(x) rank(x) / (length(x) + 1)))
# Quedandonos con 7 columnas
wine_data_subset <- wine_data_pseudo[, c("fixed.acidity", "volatile.acidity", "citric.acid",
"residual.sugar", "chlorides", "free.sulfur.dioxide",
"total.sulfur.dioxide")]
# Generar un gráfico cópula
ggpairs(
wine_data_subset,
lower = list(continuous = wrap("density", alpha = 0.7, color = "blue")),
diag = list(continuous = wrap("barDiag", color = "grey", bins = 10)),
upper = list(continuous = wrap("cor", size = 2, color = "red"))
) +
theme(
axis.text.x = element_text(angle = 90, hjust = 1),
axis.text.y = element_text(angle = 0)
)
